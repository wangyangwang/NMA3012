--- 
layout: default 
---


<h2>Interaction Design</h2>

<div>
    <h4>Kinect Integration</h4>
    {% highlight js %} 
        ////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - begin declaring variables//////////////
////////////////////////////////////////////////////////////////////////////////////

import SimpleOpenNI.*;
SimpleOpenNI context; //declare a new SimpleOpenNI object called context
float        zoomF =0.5f;
float        rotX = radians(180);  // by default rotate the hole scene 180deg around the x-axis, 
// the data from openni comes upside down
float        rotY = radians(0);
boolean      autoCalib=true;

PVector      bodyCenter = new PVector();
PVector      bodyDir = new PVector();
PVector      com = new PVector();                                   
PVector      com2d = new PVector();                                    

int distance = 3000;
PVector nearestH = new PVector(0,0, distance);  //head position that's closest to the camera
////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - end declaring variables/////////
////////////////////////////////////////////////////////////////////////////////////


import processing.video.*;

Movie mov; //declare move object
float tempPos = width/2; //stores mouse position from the previous frame
float tempSpeed = 1;

void setup() {
////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - begin initializing variables////
////////////////////////////////////////////////////////////////////////////////////
  context = new SimpleOpenNI(this); 
  if (context.isInit() == false)
  {
    println("Can't init SimpleOpenNI, maybe the camera is not connected!"); 
    exit();
    return;
  }
  
  context.setMirror(false);  // disable mirror
  context.enableDepth();  // enable depthMap generation 
  context.enableUser();   // enable skeleton generation for all joints
////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - end initializing variables//////
//////////////////////////////////////////////////////////////////////////////////// 
  size(640, 360);
  mov = new Movie(this, "tickingclock.mp4"); //initialize move object
  mov.play(); //play movie once
  mov.loop(); //loop video when finished
}

void draw() {
////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - begin reading user data/////////
////////////////////////////////////////////////////////////////////////////////////
  context.update(); 
  
    // draw the skeleton if it's available
  int[] userList = context.getUsers();
  for (int i=0; i<userList.length; i++)
  {
    if (context.isTrackingSkeleton(userList[i]))
      getBodyDirection(userList[i]);
  } 
////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - end reading user data///////////
////////////////////////////////////////////////////////////////////////////////////
    mov.read(); //start reading frames of movie according to its default sequence
    mov.speed(tempSpeed);
    image(mov, 0, 0); //draw movie at location 0, 0
    
//  interaction prototyping with mouse: control clock using mouseX position    
//  if (mouseX>width/2) {
//    mov.pause();
//  } else {
//    mov.play();
//  }


//interaction design 1: 
//as people walk passed by the clock, their relative position rewinds the clock arm.
if(nearestH.z<2000){
  println("nearestH.x"+nearestH.x);
  float tempX = map(nearestH.x, 0, 6000, 0, width);
  if(abs((tempX - tempPos))>0){
      // Ratio of mouse X over width
  float ratio = tempX / (float) width;

 // The jump() function allows you to jump immediately to a point of time within the video. 
  // duration() returns the total length of the movie in seconds.  
  mov.jump(ratio * mov.duration()); 
  }
  tempPos = mouseX;
}


//   //ineraction design 2: 
//   //when people move in front of screen, make time go faster. when people slow down, clock arm goes back to normal speed.
//    if(abs((mouseX - tempPos))>0){
//      if(tempSpeed<10){
//        tempSpeed+=0.8;
//      }
//    }else{
//      if(tempSpeed>1){
//        tempSpeed-=0.8;
//      }
//    }
//    tempPos=mouseX;
}

////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - begin data retrieving function//
////////////////////////////////////////////////////////////////////////////////////
void getBodyDirection(int userId)
{  
  PVector jointL = new PVector();
  PVector jointH = new PVector();
  PVector jointR = new PVector();
  float  confidence;

  // draw the joint position
  confidence = context.getJointPositionSkeleton(userId, SimpleOpenNI.SKEL_LEFT_SHOULDER, jointL);
  confidence = context.getJointPositionSkeleton(userId, SimpleOpenNI.SKEL_HEAD, jointH);
  confidence = context.getJointPositionSkeleton(userId, SimpleOpenNI.SKEL_RIGHT_SHOULDER, jointR);

  if(jointH.z < nearestH.z){
    nearestH = jointH;
  }
  
  
  //  // take the neck as the center point
  //  confidence = context.getJointPositionSkeleton(userId, SimpleOpenNI.SKEL_NECK, centerPoint);
  //
  //  /*  // manually calc the centerPoint
  //   PVector shoulderDist = PVector.sub(jointL,jointR);
  //   centerPoint.set(PVector.mult(shoulderDist,.5));
  //   centerPoint.add(jointR);
  //   */
  //
  //  PVector up = PVector.sub(jointH, centerPoint);

  //  PVector left = PVector.sub(jointR, centerPoint);
  //
  //  dir.set(up.cross(left));
  //  dir.normalize();
}


void onNewUser(SimpleOpenNI curContext,int userId)
{
  println("onNewUser - userId: " + userId);
  println("\tstart tracking skeleton");
  
  context.startTrackingSkeleton(userId);
}

void onLostUser(SimpleOpenNI curContext,int userId)
{
  println("onLostUser - userId: " + userId);
}

void onVisibleUser(SimpleOpenNI curContext,int userId)
{
  //println("onVisibleUser - userId: " + userId);
}
////////////////////////////////////////////////////////////////////////////////////
/////////////////////////simpleOpenNI integration - end data retriveing function////
////////////////////////////////////////////////////////////////////////////////////

    {% endhighlight %}
</div>

<div>
    <h2>Face Detection as Interface</h2>
    <div>
        <h4></h4>
    
    </div>
    <div>
        <h4>New Media Artist Adam Harvey on Viola-Jones algorithm</h4>
        <p>the Viola-Jones algorithm and the OpenCV package, what I found are these files [indicating a series of files named haar_cascade.xml and other similar things]. And you get to choose these when you implement most any face detection in Processing or OpenFrameworks. You see these weird names like “Haar cascade”, “frontal face”, “alt tree”, and there are a couple of them. These are very fine-tuned profiles for what makes up a face. Those are four that are very common. People have made a lot of other variations of those.
    <div class="row">
        <div class="col-lg-8">
            <img src="{{ '/img/week12/facedetection.jpg' | prepend: site.baseurl }}" class="img-responsive">
        </div>
    </div>
       You can see some of them I think on my screen here. There’s one that makes up an eye [called haardcascade_eye.xml]. You can get one for a full body [haarcascade_fullbody.xml], left eye, right eye, glasses, mouth, nose, basically anything. There’s even one I found for a clock. You can locate a clock in an image.
        <br><br>If you open up the XML file it’s this really hard to read syntax. In the XML file it has a node with sub-nodes and if you go all the way into the last node, you see these numbers like 3, 8, 12, 4, -1, and these things define the shape of these things called Haar features. So the Haar features are basically black and white rectangles. And the black and white rectangles can be visualized. They actually are black and white.
        <div class="row">
        <div class="col-lg-8">
            <img src="{{ '/img/week12/haarstage.jpg' | prepend: site.baseurl }}" class="img-responsive">
        </div>
    </div>
        They typically have like 20 to 30 different stages. The first stage is a very coarse scan of the image. I’ll talk about the scan in a second. But you can see in the first one here that there’s a black rectangle and a white one. And if you compare that to the face below it you can understand that the white is the highlight of the cheeks and the black is the darkness of the eyes, the shadow. And then if you look at the second one, there’s a lighter spot in the middle of the forehead and dark on the edges, which is where the hair would be. And then again on the third one the same thing as the second, but a little bit lower.
        <br><br>Well, what you do is called, I think, the Integral Sum [the Integral Image or Summed Area Table] of the part of that image. All you do is sum the black and white pixels. So all images are converted to black and white: grayscale, 0 to 255. You take these values and you create a sum of all the pixels in that area. So then you’re comparing the sum of the grayscale values of the white to the sum of the grayscale values of the black region. And that’s pretty efficient and easy to do in an image by calculating those sums. And Viola-Jones are the ones who came up with that method and it is very efficient. To really understand ityou do have to read their paper to get the formulas. It’s easier to explain with formulas.
        <iframe src="https://player.vimeo.com/video/12774628?title=0&byline=0&portrait=0" width="500" height="477" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
        This video visualizes the detection process of OpenCV's face detector. The algorithm uses the Viola Jones method of calculating the integral image and then performing some calculations on all the areas defined by the black and white rectangles to analyze the differences between the dark and light regions of a face. The sub-window (in red) is scanned across the image at various scales to detect if there is a potential face within the window. If not, it continues scanning. If it passes all stages in the cascade file, it is marked with a red rectangle. But this does not yet confirm a face. In the post-processing stage all the potential faces are checked for overlaps. Typically, 2 or 3 overlapping rectangles are required to confirm a face. Loner rectangles are rejected as false-positives.
        </p>
        
    </div>

</div>

